{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "nKyWM0tVJRuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ3XQEj8KVGm",
        "outputId": "5f1dc901-a78c-4efc-f4bb-46658d86fa7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available, otherwise use CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Install required packages\n",
        "print(\"Installing packages...\")\n",
        "!pip install clip-dissect torch torchvision pandas matplotlib numpy\n",
        "\n",
        "# Step 2: Clone the CLIP-Dissect repository\n",
        "print(\"Cloning CLIP-Dissect repository...\")\n",
        "!git clone https://github.com/Trustworthy-ML-Lab/CLIP-dissect.git\n",
        "%cd CLIP-dissect\n",
        "\n",
        "# Import necessary modules from CLIP-Dissect\n",
        "import utils\n",
        "import similarity\n",
        "import data_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_zXQ2GOJSsQ",
        "outputId": "73e63b3c-e3c6-4f96-83b2-e50d0ae70b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Installing packages...\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement clip-dissect (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for clip-dissect\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning CLIP-Dissect repository...\n",
            "Cloning into 'CLIP-dissect'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 104 (delta 12), reused 14 (delta 8), pack-reused 80 (from 1)\u001b[K\n",
            "Receiving objects: 100% (104/104), 15.94 MiB | 38.06 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/CLIP-dissect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Download and extract the Broden dataset (224x224 resolution for ResNet18)\n",
        "base_url = \"http://netdissect.csail.mit.edu/data/\"\n",
        "dataset_name = \"broden1_224\"\n",
        "zip_filename = f\"{dataset_name}.zip\"\n",
        "download_dir = \"dataset\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "full_zip_path = os.path.join(download_dir, zip_filename)\n",
        "\n",
        "if not os.path.isfile(os.path.join(download_dir, dataset_name, \"index.csv\")):\n",
        "    print(f\"Downloading {dataset_name}...\")\n",
        "    urllib.request.urlretrieve(f\"{base_url}{zip_filename}\", full_zip_path)\n",
        "    print(f\"Unzipping {zip_filename}...\")\n",
        "    with zipfile.ZipFile(full_zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(download_dir)\n",
        "    os.remove(full_zip_path)\n",
        "    print(f\"{dataset_name} downloaded and extracted to {download_dir}/{dataset_name}\")\n",
        "else:\n",
        "    print(f\"{dataset_name} already downloaded and extracted.\")\n",
        "\n",
        "# Set the dataset root path for Broden\n",
        "data_utils.DATASET_ROOTS[\"broden\"] = os.path.join(download_dir, dataset_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQZ0PJbdKA0M",
        "outputId": "0b66bbe8-04dd-43c6-c617-a5419a1e0a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading broden1_224...\n",
            "Unzipping broden1_224.zip...\n",
            "broden1_224 downloaded and extracted to dataset/broden1_224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Broden dataset structure\n",
        "broden_path = os.path.join(download_dir, dataset_name)\n",
        "if not os.path.isfile(os.path.join(broden_path, \"index.csv\")):\n",
        "    print(f\"Error: Broden dataset at {broden_path} is missing index.csv\")\n",
        "    raise FileNotFoundError(\"Broden dataset is not properly set up.\")\n",
        "print(f\"Broden dataset verified at {broden_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qaH15tOY4b7",
        "outputId": "1481b029-1e5f-4da8-eec8-db2c512dcd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broden dataset verified at dataset/broden1_224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for image directories\n",
        "image_dirs = [d for d in os.listdir(broden_path) if os.path.isdir(os.path.join(broden_path, d))]\n",
        "print(f\"Broden dataset verified at {broden_path}. Found image directories: {image_dirs}\")\n",
        "\n",
        "# Set the dataset root path for Broden\n",
        "data_utils.DATASET_ROOTS[\"broden\"] = broden_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnT-w88OLzyn",
        "outputId": "70a8e423-067b-4bde-c8cf-12f910574ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broden dataset verified at dataset/broden1_224. Found image directories: ['images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create a simple concept set file (20k.txt)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "concepts = [\n",
        "    \"dog\", \"cat\", \"car\", \"tree\", \"mountain\", \"sky\", \"water\",\n",
        "    \"building\", \"grass\", \"road\", \"house\", \"forest\", \"river\",\n",
        "    \"person\", \"bird\", \"cloud\", \"chair\", \"table\", \"window\", \"door\"\n",
        "]  # Small list for demo; expand with full Broden concepts for full analysis\n",
        "concept_set_path = \"data/20k.txt\"\n",
        "with open(concept_set_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(concepts))\n",
        "print(f\"Concept set created at {concept_set_path} with {len(concepts)} concepts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpBXip_ZL_2d",
        "outputId": "273d37d9-121a-4fa4-a51a-424e3991798e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concept set created at data/20k.txt with 20 concepts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Set up basic settings for the task\n",
        "clip_model = \"ViT-B/16\"  # CLIP model for descriptions\n",
        "target_layers = [\"layer2\", \"layer3\", \"layer4\"]  # Layers to dissect in ResNet18\n",
        "d_probe = \"broden\"  # Dataset to use\n",
        "batch_size = 50  # Reduced to avoid memory issues\n",
        "activation_dir = \"saved_activations\"  # Where to save activations\n",
        "result_dir = \"results\"  # Where to save results\n",
        "pool_mode = \"avg\"  # Pooling mode for activations\n",
        "similarity_fn_name = \"soft_wpmi\"  # Similarity function"
      ],
      "metadata": {
        "id": "v3yQrbSGNAwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load ResNet18 models\n",
        "# ImageNet-trained ResNet18\n",
        "print(\"Loading ResNet18 for ImageNet...\")\n",
        "model_imagenet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).to(device)\n",
        "model_imagenet.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6py06sLLTl_s",
        "outputId": "ae86b24b-1347-41f7-b85a-c5972023fa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ResNet18 for ImageNet...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Places365-trained ResNet18\n",
        "print(\"Loading ResNet18 for Places365...\")\n",
        "model_places = models.resnet18(weights=None)\n",
        "model_places.fc = nn.Linear(model_places.fc.in_features, 365)  # Adjust for Places365 classes\n",
        "places_url = \"http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRbW-xV4Rnkm",
        "outputId": "e2e23a3c-7b32-47a2-b8bf-82ea0f2d7fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ResNet18 for Places365...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    checkpoint = torch.hub.load_state_dict_from_url(places_url, progress=True)\n",
        "    state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "    model_places.load_state_dict(state_dict)\n",
        "    model_places = model_places.to(device)\n",
        "    model_places.eval()\n",
        "    print(\"Places365 model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Places365 model: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiPJqjlWRwZ2",
        "outputId": "1bbf8832-cf11-4033-89f6-af639af87397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Places365 model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import json\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import json\n",
        "import clip\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "9zddrYczbYjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Verify dataset loading and save activations for both models\n",
        "os.makedirs(activation_dir, exist_ok=True)\n",
        "\n",
        "# Define preprocessing transform for Broden dataset\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Verify dataset loading\n",
        "try:\n",
        "    print(f\"Testing dataset loading for {d_probe}...\")\n",
        "    dataset = data_utils.get_data(d_probe, preprocess=preprocess)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    print(f\"Dataset loaded successfully. Number of samples: {len(dataset)}\")\n",
        "    for batch in dataloader:\n",
        "        images, _ = batch\n",
        "        print(f\"Sample batch shape: {images.shape}\")\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset {d_probe}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Custom function to extract and save activations\n",
        "def custom_save_activations(model, model_name, clip_model_name, target_layers, dataloader, concept_set_path, device, save_dir, pool_mode=\"avg\"):\n",
        "    # Load CLIP model\n",
        "    clip_model, clip_preprocess = clip.load(clip_model_name, device=device)\n",
        "    clip_model.eval()\n",
        "\n",
        "    # Read concepts\n",
        "    with open(concept_set_path, 'r') as f:\n",
        "        concepts = f.read().splitlines()\n",
        "\n",
        "    # Hook to capture activations\n",
        "    activations = {}\n",
        "    def hook_fn(module, input, output, layer_name):\n",
        "        if pool_mode == \"avg\":\n",
        "            activations[layer_name] = output.mean(dim=(2, 3))  # Average pooling over spatial dimensions\n",
        "        else:\n",
        "            activations[layer_name] = output.max(dim=2)[0].max(dim=2)[0]  # Max pooling\n",
        "\n",
        "    # Register hooks for target layers\n",
        "    hooks = []\n",
        "    for layer_name in target_layers:\n",
        "        layer = getattr(model, layer_name)\n",
        "        hook = layer.register_forward_hook(lambda m, i, o, ln=layer_name: hook_fn(m, i, o, ln))\n",
        "        hooks.append(hook)\n",
        "\n",
        "    # Process dataset to collect target model activations\n",
        "    target_acts = {layer: [] for layer in target_layers}\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = images.to(device)\n",
        "            activations.clear()\n",
        "            model(images)  # Forward pass to trigger hooks\n",
        "            for layer in target_layers:\n",
        "                target_acts[layer].append(activations[layer].cpu())\n",
        "\n",
        "    # Concatenate activations\n",
        "    for layer in target_layers:\n",
        "        target_acts[layer] = torch.cat(target_acts[layer], dim=0)\n",
        "\n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Process images through CLIP for visual features\n",
        "    clip_acts = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            clip_images = torch.stack([clip_preprocess(Image.fromarray((img.permute(1, 2, 0).numpy() * 255).astype(np.uint8))) for img in images])\n",
        "            clip_images = clip_images.to(device)\n",
        "            clip_acts.append(clip_model.encode_image(clip_images).cpu())\n",
        "    clip_acts = torch.cat(clip_acts, dim=0)\n",
        "\n",
        "    # Process text (concepts) through CLIP\n",
        "    text_inputs = clip.tokenize(concepts).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_acts = clip_model.encode_text(text_inputs).cpu()\n",
        "\n",
        "    # Save activations in CLIP-Dissect format\n",
        "    for layer in target_layers:\n",
        "        torch.save(target_acts[layer], os.path.join(save_dir, f\"broden_{model_name}_{layer}.pt\"))\n",
        "        torch.save(clip_acts, os.path.join(save_dir, f\"broden_{model_name}_{layer}_clip.pt\"))\n",
        "        torch.save(text_acts, os.path.join(save_dir, f\"broden_{model_name}_text.pt\"))\n",
        "\n",
        "# Save activations using custom function\n",
        "for model_name, model in [(\"resnet18\", model_imagenet), (\"resnet18_places\", model_places)]:\n",
        "    print(f\"Saving activations for {model_name}...\")\n",
        "    try:\n",
        "        print(f\"Target layers for {model_name}: {target_layers}\")\n",
        "        custom_save_activations(\n",
        "            model=model,\n",
        "            model_name=model_name,\n",
        "            clip_model_name=clip_model,\n",
        "            target_layers=target_layers,\n",
        "            dataloader=dataloader,\n",
        "            concept_set_path=concept_set_path,\n",
        "            device=device,\n",
        "            save_dir=activation_dir,\n",
        "            pool_mode=pool_mode\n",
        "        )\n",
        "        # Verify that activation files were created\n",
        "        for layer in target_layers:\n",
        "            activation_file = os.path.join(activation_dir, f\"broden_{model_name}_{layer}.pt\")\n",
        "            clip_file = os.path.join(activation_dir, f\"broden_{model_name}_{layer}_clip.pt\")\n",
        "            text_file = os.path.join(activation_dir, f\"broden_{model_name}_text.pt\")\n",
        "            if os.path.isfile(activation_file) and os.path.isfile(clip_file) and os.path.isfile(text_file):\n",
        "                print(f\"Activation files created: {activation_file}, {clip_file}, {text_file}\")\n",
        "            else:\n",
        "                print(f\"Error: Activation files missing for {model_name}, layer {layer}\")\n",
        "                raise FileNotFoundError(f\"Failed to save activations for {model_name}, layer {layer}\")\n",
        "        print(f\"Activations saved successfully for {model_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving activations for {model_name}: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZXt3F7IR-4A",
        "outputId": "b8e5bc3f-3ead-4078-bdf9-35755f273cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dataset loading for broden...\n",
            "Dataset loaded successfully. Number of samples: 210365\n",
            "Sample batch shape: torch.Size([50, 3, 224, 224])\n",
            "Saving activations for resnet18...\n",
            "Target layers for resnet18: ['layer2', 'layer3', 'layer4']\n",
            "Activation files created: saved_activations/broden_resnet18_layer2.pt, saved_activations/broden_resnet18_layer2_clip.pt, saved_activations/broden_resnet18_text.pt\n",
            "Activation files created: saved_activations/broden_resnet18_layer3.pt, saved_activations/broden_resnet18_layer3_clip.pt, saved_activations/broden_resnet18_text.pt\n",
            "Activation files created: saved_activations/broden_resnet18_layer4.pt, saved_activations/broden_resnet18_layer4_clip.pt, saved_activations/broden_resnet18_text.pt\n",
            "Activations saved successfully for resnet18.\n",
            "Saving activations for resnet18_places...\n",
            "Target layers for resnet18_places: ['layer2', 'layer3', 'layer4']\n",
            "Activation files created: saved_activations/broden_resnet18_places_layer2.pt, saved_activations/broden_resnet18_places_layer2_clip.pt, saved_activations/broden_resnet18_places_text.pt\n",
            "Activation files created: saved_activations/broden_resnet18_places_layer3.pt, saved_activations/broden_resnet18_places_layer3_clip.pt, saved_activations/broden_resnet18_places_text.pt\n",
            "Activation files created: saved_activations/broden_resnet18_places_layer4.pt, saved_activations/broden_resnet18_places_layer4_clip.pt, saved_activations/broden_resnet18_places_text.pt\n",
            "Activations saved successfully for resnet18_places.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Collect results for both models\n",
        "similarity_fn = getattr(similarity, similarity_fn_name)\n",
        "with open(concept_set_path, 'r') as f:\n",
        "    words = f.read().splitlines()\n",
        "\n",
        "for model_name in [\"resnet18\", \"resnet18_places\"]:\n",
        "    print(f\"Collecting results for {model_name}...\")\n",
        "    outputs = {\"layer\": [], \"unit\": [], \"description\": [], \"similarity\": []}\n",
        "    for target_layer in target_layers:\n",
        "        save_names = utils.get_save_names(\n",
        "            clip_name=clip_model,\n",
        "            target_name=model_name,\n",
        "            target_layer=target_layer,\n",
        "            d_probe=d_probe,\n",
        "            concept_set=concept_set_path,\n",
        "            pool_mode=pool_mode,\n",
        "            save_dir=activation_dir\n",
        "        )\n",
        "        target_save_name, clip_save_name, text_save_name = save_names\n",
        "\n",
        "        if not os.path.isfile(target_save_name):\n",
        "            print(f\"Error: Activation file {target_save_name} not found for {model_name}.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            similarities = utils.get_similarity_from_activations(\n",
        "                target_save_name, clip_save_name, text_save_name, similarity_fn,\n",
        "                return_target_feats=False, device=device\n",
        "            )\n",
        "            vals, ids = torch.max(similarities, dim=1)\n",
        "            descriptions = [words[int(idx)] for idx in ids]\n",
        "\n",
        "            outputs[\"unit\"].extend(range(len(vals)))\n",
        "            outputs[\"layer\"].extend([target_layer] * len(vals))\n",
        "            outputs[\"description\"].extend(descriptions)\n",
        "            outputs[\"similarity\"].extend(vals.cpu().numpy())\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing similarities for {model_name}, layer {target_layer}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    df = pd.DataFrame(outputs)\n",
        "    timestamp = datetime.datetime.now().strftime('%y_%m_%d_%H_%M')\n",
        "    save_path = os.path.join(result_dir, f\"{model_name}_{timestamp}\")\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    csv_path = os.path.join(save_path, \"descriptions.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Results saved for {model_name} at {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rZ6Jrr_T4eM",
        "outputId": "77228c44-b14b-4e94-be8a-51a0d681dbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting results for resnet18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 128/128 [00:00<00:00, 1439.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:00<00:00, 6636.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:00<00:00, 6752.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 20])\n",
            "Results saved for resnet18 at results/resnet18_25_07_21_20_32/descriptions.csv\n",
            "Collecting results for resnet18_places...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 128/128 [00:00<00:00, 6404.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [00:00<00:00, 6757.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:00<00:00, 6693.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 20])\n",
            "Results saved for resnet18_places at results/resnet18_places_25_07_21_20_32/descriptions.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Load results for analysis\n",
        "try:\n",
        "    latest_imagenet_dir = max([d for d in os.listdir(result_dir) if \"resnet18_\" in d and not \"places\" in d], key=lambda x: os.path.getctime(os.path.join(result_dir, x)))\n",
        "    latest_places_dir = max([d for d in os.listdir(result_dir) if \"resnet18_places\" in d], key=lambda x: os.path.getctime(os.path.join(result_dir, x)))\n",
        "\n",
        "    df_imagenet = pd.read_csv(os.path.join(result_dir, latest_imagenet_dir, \"descriptions.csv\"))\n",
        "    df_places = pd.read_csv(os.path.join(result_dir, latest_places_dir, \"descriptions.csv\"))\n",
        "    print(\"Result CSV files loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading result CSV files: {e}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSs_p57yXBb-",
        "outputId": "f75e3d85-0d76-4a21-cde9-660161832b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result CSV files loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Analyze results\n",
        "# Most common concepts\n",
        "print(\"\\nTop 5 concepts for ResNet18 (ImageNet):\")\n",
        "imagenet_counts = Counter(df_imagenet['description'])\n",
        "for concept, count in imagenet_counts.most_common(5):\n",
        "    print(f\"{concept}: {count} neurons\")\n",
        "\n",
        "print(\"\\nTop 5 concepts for ResNet18 (Places365):\")\n",
        "places_counts = Counter(df_places['description'])\n",
        "for concept, count in places_counts.most_common(5):\n",
        "    print(f\"{concept}: {count} neurons\")\n",
        "\n",
        "# Compare concepts between models\n",
        "imagenet_set = set(imagenet_counts.keys())\n",
        "places_set = set(places_counts.keys())\n",
        "common_concepts = imagenet_set.intersection(places_set)\n",
        "unique_imagenet = imagenet_set - places_set\n",
        "unique_places = places_set - imagenet_set\n",
        "print(f\"\\nCommon concepts between models: {len(common_concepts)}\")\n",
        "print(f\"Concepts unique to ImageNet: {len(unique_imagenet)}\")\n",
        "print(f\"Concepts unique to Places365: {len(unique_places)}\")\n",
        "\n",
        "# Count unique objects (excluding scene-like concepts)\n",
        "non_object_concepts = ['sky', 'grass', 'road', 'mountain', 'water', 'cloud', 'forest', 'river']\n",
        "imagenet_objects = len([c for c in imagenet_counts.keys() if c not in non_object_concepts])\n",
        "places_objects = len([c for c in places_counts.keys() if c not in non_object_concepts])\n",
        "print(f\"\\nUnique objects in ImageNet: {imagenet_objects}\")\n",
        "print(f\"Unique objects in Places365: {places_objects}\")\n",
        "\n",
        "# Plot similarity score distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_imagenet['similarity'], bins=30, alpha=0.5, label='ImageNet', color='blue')\n",
        "plt.hist(df_places['similarity'], bins=30, alpha=0.5, label='Places365', color='green')\n",
        "plt.xlabel('Similarity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Similarity Score Distribution')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(result_dir, 'similarity_distribution.png'))\n",
        "plt.close()\n",
        "print(\"Similarity distribution plot saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSgTXB9QhqSA",
        "outputId": "b038121b-bd2b-499d-8c5e-c3a519df9ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 concepts for ResNet18 (ImageNet):\n",
            "cloud: 246 neurons\n",
            "chair: 90 neurons\n",
            "person: 86 neurons\n",
            "tree: 80 neurons\n",
            "house: 67 neurons\n",
            "\n",
            "Top 5 concepts for ResNet18 (Places365):\n",
            "person: 180 neurons\n",
            "sky: 151 neurons\n",
            "cloud: 116 neurons\n",
            "chair: 91 neurons\n",
            "river: 62 neurons\n",
            "\n",
            "Common concepts between models: 20\n",
            "Concepts unique to ImageNet: 0\n",
            "Concepts unique to Places365: 0\n",
            "\n",
            "Unique objects in ImageNet: 12\n",
            "Unique objects in Places365: 12\n",
            "Similarity distribution plot saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot layer-wise concept counts\n",
        "for df, model_name in [(df_imagenet, \"ResNet18_ImageNet\"), (df_places, \"ResNet18_Places365\")]:\n",
        "    layer_counts = df.groupby('layer')['description'].value_counts().unstack(fill_value=0)\n",
        "    layer_counts.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "    plt.title(f'Concept Distribution by Layer - {model_name}')\n",
        "    plt.xlabel('Layer')\n",
        "    plt.ylabel('Number of Neurons')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(result_dir, f'{model_name}_layer_concepts.png'))\n",
        "    plt.close()\n",
        "    print(f\"Layer-wise concept plot saved for {model_name}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9TDAEVkhyCO",
        "outputId": "cc1b6d83-8723-4fde-ab61-050c6d92ebbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer-wise concept plot saved for ResNet18_ImageNet.\n",
            "Layer-wise concept plot saved for ResNet18_Places365.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save analysis summary\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "with open(os.path.join(result_dir, 'analysis_summary.txt'), 'w') as f:\n",
        "    f.write(\"Network Dissection Analysis Summary\\n\")\n",
        "    f.write(\"==============================\\n\")\n",
        "    f.write(f\"Top 5 concepts (ImageNet):\\n{json.dumps(dict(imagenet_counts.most_common(5)), indent=2)}\\n\")\n",
        "    f.write(f\"Top 5 concepts (Places365):\\n{json.dumps(dict(places_counts.most_common(5)), indent=2)}\\n\")\n",
        "    f.write(f\"Common concepts: {len(common_concepts)}\\n\")\n",
        "    f.write(f\"Unique to ImageNet: {len(unique_imagenet)}\\n\")\n",
        "    f.write(f\"Unique to Places365: {len(unique_places)}\\n\")\n",
        "    f.write(f\"Unique objects (ImageNet): {imagenet_objects}\\n\")\n",
        "    f.write(f\"Unique objects (Places365): {places_objects}\\n\")\n",
        "print(\"Analysis summary saved at results/analysis_summary.txt\")\n",
        "\n",
        "print(\"Analysis complete. Check the 'results' directory for outputs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kbbbCkc2dmA",
        "outputId": "bc1bfd41-8270-4960-83c5-d8927ddf896a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis summary saved at results/analysis_summary.txt\n",
            "Analysis complete. Check the 'results' directory for outputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define source and destination paths\n",
        "source_dir = 'results'\n",
        "dest_dir = '/content/drive/MyDrive/CLIP_Dissect_Results'\n",
        "\n",
        "# Copy the results directory to Google Drive\n",
        "if os.path.exists(source_dir):\n",
        "    shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)\n",
        "    print(f\"Results directory copied to {dest_dir}\")\n",
        "else:\n",
        "    print(f\"Error: {source_dir} does not exist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQKbSL822n6t",
        "outputId": "e9feeff7-6cc2-49f0-847d-672b4526995e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Results directory copied to /content/drive/MyDrive/CLIP_Dissect_Results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bArh0S7X3yGZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fp5SVBWU-8Ku"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}